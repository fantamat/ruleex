from gtrain.utils import confmat
from ruleex.hypinv.data import DataForBoundary
from ruleex.hypinv.model import NetForHypinv
from ruleex.tree.build import build_from_rules
from ruleex.tree.rule import *
from ruleex.tree.ruletree import RuleTree
import tensorflow as tf

#_______________________________________________CONSTANTS_DECLARATION______________________________________________

# PARAMS CONSTANTS
from gtrain.core import gtrain, strain

MAX_NUM_RULES = "max_rules"
DEFAULT_MAX_NUM_RULES = 10
MAX_DEPTH = "max_depth"
DEFAULT_MAX_DEPTH = None
THRESH_FIDELITY = "thresh_fidelity"
DEFAULT_THRESH_FIDELITY = 0.95
# Evolutionary is not supported yet
    #USE_EVOLUTIONARY = useEvolutionary
    #DEFAULT_USE_EVOLUTIONARY = False
    #INITIAL_BOUNDARY_POINTS = "initialBoundaryPoints"
    #DEFAULT_INITIAL_BOUNDARY_PINTS = 1000
ENABLE_CENTER_INITIAL_POINT = "center_initial_point"
DEFAULT_ENABLE_CENTER_INITIAL_POINT = True
USE_TRAINING_POINTS = "use_training_points"
DEFAULT_USE_TRAINING_POINTS = True
WEIGHT_BY_CENTER = "weight_by_center"
DEFAULT_WEIGHT_BY_CENTER = True
ALPHA = "alpha"
DEFAULT_ALPHA = 0.01
U_THRESHOLD = "uThreshold"
DEFAULT_U_THRESHOLD = 0.01
MAX_SLIDING_STEPS = "max_sliding_steps"
DEFAULT_MAX_SLIDING_STEPS = 50
BOUNDARY_WIDTH = "boundary_width"
DEFAULT_BOUNDARY_WIDTH = 0.01
GTRAIN_PARAMS = "gtrain_params"
MOOVING_STEPS = "mooving_steps"
DEFAULT_MOOVING_STEPS = 10
DEFAULT_GTRAIN_PARAMS = {
    "max_fails": 2,
    "num_steps": 1000,
    "evaluate_every": 101,
    "checkpoint_every": 101,
    "dtype": tf.float64
}


DEFAULT_PARAMS = {MAX_NUM_RULES: DEFAULT_MAX_NUM_RULES,
                  THRESH_FIDELITY: DEFAULT_THRESH_FIDELITY, 
                  #INITIAL_BOUNDARY_POINTS: DEFAULT_INITIAL_BOUNDARY_PINTS,
                  ENABLE_CENTER_INITIAL_POINT: DEFAULT_ENABLE_CENTER_INITIAL_POINT,
                  USE_TRAINING_POINTS: DEFAULT_USE_TRAINING_POINTS,
                  ALPHA: DEFAULT_ALPHA,
                  U_THRESHOLD: DEFAULT_U_THRESHOLD,
                  MAX_SLIDING_STEPS: DEFAULT_MAX_SLIDING_STEPS,
                  BOUNDARY_WIDTH: DEFAULT_BOUNDARY_WIDTH,
                  GTRAIN_PARAMS: DEFAULT_GTRAIN_PARAMS,
                  WEIGHT_BY_CENTER: DEFAULT_WEIGHT_BY_CENTER,
                  MAX_DEPTH: DEFAULT_MAX_DEPTH,
                  MOOVING_STEPS: DEFAULT_MOOVING_STEPS,
                  }

# INFO key
INF = "inf"
# INFO CONSTANTS
CENTER = "center"
POINTS = "points"
BOUNDARY_POINTS = "boundary_points"
FIDELITIES = "fidelities"
SLIDE_STEPS = "slide_steps"

#_______________________________________________CORE_FUNCTION_FOR_ALGORITHM__________________________________________




def find_closest(x1, samples_of_boundary_points, model, model_x1_classification, params):
    """
    Finds the closest point to the x1 on the decision boundary for its class model_x1_classification
    :param x1: a point to which the closest on the boundary is found
    :param samples_of_boundary_points: samples generated by evolutionary algorithm that lies on the decision boundary
    :param model:
    :param model_x1_classification: classificaiton of the x1 by the model
    :param params: parameters derived from the hypinv params
    :return: a closest point on the boundary, i.e., x0
    """
    data = DataForBoundary()
    model.set_train_class(model_x1_classification)
    if CENTER in params:
        model.set_center(params[CENTER])

    if model.has_modified_loss():
        model.set_initial_x(x1)
        model.set_x1(x1)
        gtrain(model, data, **params[GTRAIN_PARAMS])
        return model.trained_x, {"modified_loss": True}

    def return_x_to_boundary(x, num_loops, num_steps, session):
        model.set_initial_x_in_session(x[0], session)
        counter = 0
        out = None
        while counter < num_loops:
            counter += 1
            session = strain(model, data, optimizer=tf.compat.v1.train.AdamOptimizer(), num_steps=num_steps, session=session)
            x0 = model.trained_x
            g = model.get_boundary_gradient(x0, model_x1_classification)
            norm = np.linalg.norm(g)
            y = model.boundary_eval(x0, model_x1_classification)
            print("moving back {}".format(np.abs(y[0, 0] - 0.5)))
            if norm != 0 and np.abs(y[0, 0] - 0.5) < params[BOUNDARY_WIDTH]:
                out = x0
        if out is None:
            return session, x0
        else:
            return session, out

    session, x0 = return_x_to_boundary([x1], 20, 5*params[MOOVING_STEPS], None)
    g = model.get_boundary_gradient(x0, model_x1_classification)
    norm = np.linalg.norm(g)
    step_points = [x0]
    norms = []
    shrinks_steps = [0]
    boundary_returns = [0]
    for i in range(params[MAX_SLIDING_STEPS]):
        norms.append(np.linalg.norm(x0-x1))
        print("norm(x1-x0) = {}".format(norms[-1]))

        if norm == 0:
            print("[hypinv] sliding ended because the gradient is zero vector.")
            break
        n = g/norm
        v = x1 - x0
        u = v - v*n
        print(np.linalg.norm(u))
        dx = params[ALPHA]*u
        if np.linalg.norm(u) < params[U_THRESHOLD]:
            break
        for i in range(10):
            g = model.get_boundary_gradient(x0+dx/2**i, model_x1_classification)
            norm = np.linalg.norm(g)
            if norm != 0:
                break
        dx = dx/2**i
        x0 += dx
        shrinks_steps.append(i)
        step_points.append(x0)
        boundary_returns.append(0)
        y = model.boundary_eval(x0, model_x1_classification)
        while np.abs(y[0, 0] - 0.5) > params[BOUNDARY_WIDTH]:
            boundary_returns[-1] += 1
            session, x0_new = return_x_to_boundary(x0, 10, params[MOOVING_STEPS], session)
            x0 = x0_new
            step_points[-1] = x0
            g = model.get_boundary_gradient(x0, model_x1_classification)
            norm = np.linalg.norm(g)
            y = model.boundary_eval(x0, model_x1_classification)
    best_index = np.argmin(norms)
    print("[hypinv] MAX_SLIDING_STEPS was reached.")
    return step_points[best_index], {"best_index": best_index,
                                     "modified_loss": False,
                                     "step_points": step_points,
                                     "shrinks_steps": shrinks_steps,
                                     "boundary_returns": boundary_returns}


def comp_fidelity(output, X, model_y):
    """
    :param output: RuleTree
    :param X: a lst of samples
    :param model_y: output of the model for samples X
    :return: fidelity
    """
    tree_y = output.eval_all(X)
    return np.mean(model_y == tree_y)


def fill_default_prams(params):
    for default_param in DEFAULT_PARAMS:
        if default_param not in params:
            params[default_param] = DEFAULT_PARAMS[default_param]


#______________________________________CORE_ALGORITHM__________________________________________________________


def hypinv(model, x, params=dict(), input_range=None):
    """
    Rule extraction algorithm that create oblique decision tree, i.e., RuleTree with LinearRule as its inner nodes.
    Warning: The evolutionary algorithm is not supported.

    :param model: an object with subclass NetForHypinv
        it also determine if the modified loss is used by return value of the has_modified_loss function
    :param x: a list of training samples
    :param params: a dictionary with parameters, keys are specified by constants of this module.
        MAX_NUM_RULES: maximal number of generated splits
        THRESH_FIDELITY: maximal fidelity, if the RuleTree exceed it then the algorithm stops
        ENABLE_CENTER_INITIAL_POINT: a flag that tells algorithm to use center point as initial x1
        USE_TRAINING_POINTS: if True then the next point x1 is selected forom training points
            otherwise it is randomly generated
        ALPHA: a factor of the step in the sliding process
        U_THRESHOLD: threshold for the u in the sliding process
        MAX_SLIDING_STEPS: maximal steps in the sliding process
        BOUNDARY_WIDTH: the value that defines distance from the true boundary that is also considered as its part
        GTRAIN_PARAMS: gtrain parameters used in the finding the closest point x0 by modified cost function
        WEIGHT_BY_CENTER: if True than the step in the gtrain process is weighted by the center point
        MAX_DEPTH: maximal depth of the builded RuleTree
    :param input_range: a limit for each attribute of the input
    :return: RuleTree
        - also informations about the process are stored in the params[INF]
    """

    if not isinstance(model, NetForHypinv):
        raise TypeError("model must be of the NetForHypinv type!")
    y = model.eval(x)
    labels = np.argmax(model.eval(x), axis=1)
    num_classes = len(y[0])

    def sample_boundary_points(model, inputRange, params):
        """
        # generate equidistantly distributed random samples of the input space as variable 'random_points'
        random_points = np.random.rand(params[INITIAL_BOUNDARY_POINTS], len(inputRange))
        minims = np.array(inputRange[:, 0])
        maxims = np.array(inputRange[:, 1])
        length = maxims-minims
        random_points = np.apply_along_axis(lambda point: point*length-minims, 1, random_points)

        # inversion of NN for each point in 'random_points' variable as initial point (for each class)
        output = list()
        zero_output = np.zeros(params[NUM_OF_CLASSES])
        for i, _ in enumerate(zero_output):
            # make NN inversion for all points random_points
            desired_output = zero_output
            desired_output[i] = 0.5
            data = DataForBoundary(np.tile(desired_output, (len(random_points), 1)))
            model.set_initial_x(random_points)
            gtrain(model, data, num_epochs=100, lr_max=1e6)
            output.append(model.trained_x)
            """
        return []

    def get_init_point(output, model, x, inputRange, model_y, params):
        # returns initial point x1
        if params[ENABLE_CENTER_INITIAL_POINT]:
            center = np.mean(x, 0)
            params[CENTER] = center
            model_classification = np.argmax(model.eval(center))
            if model_classification != 0:
                return center, model_classification, 0
            else:
                output.root.class_set = {1}
                return center, 0, 1
        else:
            return choose_x1(output, x, inputRange, model_y, params)

    def choose_x1(output, X, model, inputRange, model_y, params):
        # select a new point x1
        def core(points, model_class, tree_class):
            cm = confmat(model_class, tree_class, num_classes)
            for i in range(num_classes):
                cm[i, i] = 0
            if np.sum(cm) == 0:
                return None
            max_index = np.argmax(cm)
            r = np.random.randint(0,np.max(cm))
            inds = list()
            for i, x1 in enumerate(points):
                if model_class[i] == max_index // num_classes and tree_class[i] == max_index % num_classes:
                    inds.append(i)
            return points[inds[r]], max_index // num_classes, max_index % num_classes
            # return np.mean(points[inds], axis=0), max_index // num_classes, max_index % num_classes
            return None, None, None

        if params[USE_TRAINING_POINTS]:
            tree_classification = output.eval_all(X)
            return core(X, model_y, tree_classification)
        else:
            def interval_random(r, i):
                return (i[1, :] - i[0, :]) * r + i[0, :]
            for _ in range(1000):  # maximum of 100 000 points will be generated
                # generate 100 random points
                random_points = np.apply_along_axis(interval_random, 1, np.random.rand(100, len(inputRange[0])), inputRange)
                model_classification = np.argmax(model.eval(random_points), 1)
                tree_classification = output.eval_all(random_points)
                # first badly classified point is chosen as initial
                core_output = core(random_points, model_classification, tree_classification)
                if core_output:
                    return core_output

    rule_counter = 0
    if input_range is None:
        input_range = np.array([np.min(x, 0), np.max(x, 0)])
        input_range = np.transpose(input_range)
    fill_default_prams(params)

    output = RuleTree(max(labels) + 1, len(x[0]))
    output.root.class_set = {0}

    model_labels = np.argmax(model.eval(x), 1)

    # declare information variables
    points = list()
    boundary_points = list()
    fidelities = list()
    slide_steps = list()

    x1, model_x1_classification, tree_x1_classification = get_init_point(output, model, x, input_range, model_labels, params)
    points.append(x1)

    samples_of_boundary_points = sample_boundary_points(model, input_range, params)
    split_set = list()

    for i in range(params[MAX_NUM_RULES]):
        x0, steps = find_closest(x1, samples_of_boundary_points, model, model_x1_classification, params) # point on the decision boundary
        boundary_points.append(x0)
        slide_steps.append(steps)
        n = x1 - x0 # normal vector
        new_rule = LinearRule(n[0], np.sum(n*x0))
        split_set.append(new_rule)
        output = build_from_rules(split_set, x, labels, num_of_classes=num_classes, max_depth=params[MAX_DEPTH])
        rule_counter += 1
        fidelity = comp_fidelity(output, x, model_labels)
        fidelities.append(fidelity)
        print("[hypinv]: step: {} fidelity: {}".format(i, fidelity))
        if fidelity > params[THRESH_FIDELITY]:
            print("[hypinv]: Fidelity extends desired threshold.")
            break
        x1, model_x1_classification, tree_x1_classification = choose_x1(output, x, model, input_range, model_labels, params)
        if isinstance(x1, type(None)):
            print("[hypinv]: There is no more miss classified point so the algorithm will be stopped.")
            break
        points.append(x1)

    output.fill_class_sets()
    output.delete_redundancy()
    info = dict()
    info[POINTS] = points
    info[BOUNDARY_POINTS] = points
    info[FIDELITIES] = fidelities
    info[SLIDE_STEPS] = slide_steps
    params[INF] = info
    return output


