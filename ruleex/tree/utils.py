import numpy as np

from ruleex.tree.ruletree import RuleTree
from ruleex.tree.rule import Leaf, AxisRule, LinearRule


def sklearndt_to_ruletree(skDT: object, one_class_on_leafs: bool = False, min_split_fraction: float = 0.02, min_rule_samples: int = 1) -> RuleTree:
    """
    Transformation of the DT from scikit-learn to RuleTree object
    :param skDT: scikit-learn decision tree (sklearn.tree.DecisionTreeClassifier)
    :param one_class_on_leafs: defines wheater the output RuleTree has only one class sin the Leaf's class_set
    :param min_split_fraction: minimal ration of the samples on the split nodes
    :param min_rule_samples: minimal number of the samples on the split nodes
    :return: pruned copy of the skDT as RuleTree object
    """
    result = RuleTree(skDT.n_classes_,skDT.max_features_)
    generatedRules = list()
    tree = skDT.tree_
    #build rules
    for i in range(tree.node_count):
        if (tree.children_left[i]==tree.children_right[i]):
            generatedRules.append(Leaf([]))
        else:
            generatedRules.append(AxisRule(int(tree.feature[i]), float(tree.threshold[i]))) # left is false
        classSet = list()
        class_hits = [0]*skDT.n_classes_
        for j in range(len(tree.value[i][0])):
            if tree.value[i][0][j] > 0:
                classSet.append(j)
                class_hits[j] = tree.value[i][0][j]
        if one_class_on_leafs:
            generatedRules[i].class_set = {np.argmax(class_hits)}
        else:
            generatedRules[i].class_set = set(classSet)
        generatedRules[i].class_hits = class_hits

    #build conections
    for i in range(tree.node_count):
        if (tree.children_left[i] != tree.children_right[i]):
            generatedRules[i].true_branch = generatedRules[tree.children_right[i]]
            generatedRules[i].false_branch = generatedRules[tree.children_left[i]]
    result.root = generatedRules[0]
    result.prune_using_hits(min_split_fraction, min_rule_samples)

    result.delete_redundancy()
    return result


def pybliquedt_to_ruletree(pybliquedt: dict, num_of_classes: int) -> RuleTree:
    """
    Conversion of oblique decision tree (OC1) obtained from python project available on https://github.com/KDercksen/pyblique
    More information about OC1 algorithm in
    @article{murthy1994system,
      title={A system for induction of oblique decision trees},
      author={Murthy, Sreerama K and Kasif, Simon and Salzberg, Steven},
      journal={Journal of artificial intelligence research},
      volume={2},
      pages={1--32},
      year={1994}
    }
    Use case:
        from pyblique import ObliqueClassifier
        ...
        oc = ObliqueClassifier()
        oc.fit(data)
        rule_tree = pybliquedt_to_ruletree(oc.tree)

    :param pybliquedt: representation of the obligue decision tree.
        Dictionary with keys:
            "split" np array with length of (input_size + 1) - represents function f(x) = sum(np.dot("split"[:-2],x))+"split"[-1]
            "low" dictionary of other node or float whitch represents leaf node (branch for samples that have f(x) lower than zero
            "high" same as "low" just for samples that have f(x) higher than zero
    :param num_of_classes: number of classes
    :return: RuleTree object as a copy of input oblique tree
    """
    SPLIT = "split"
    LOW = "low"
    HIGH = "high"
    HITS_LOW = "class_hits_low"
    HITS_HIGH = "class_hits_high"

    result = RuleTree(num_of_classes, len(pybliquedt[SPLIT])-1)
    result.type = "Oblique decision tree (generated by OC1 algorithm)"

    def _generate_subtree(root):
        new_rule = LinearRule(root[SPLIT][:-1], -root[SPLIT][-1])
        if type(root[HIGH]) is not dict:
            new_rule.true_branch = Leaf({int(root[HIGH])-1})
            if HITS_HIGH in root:
                class_hits = np.zeros(num_of_classes)
                class_hits[:len(root[HITS_HIGH])] = root[HITS_HIGH]
                new_rule.true_branch.class_hits = list(map(int, class_hits))
        else:
            new_rule.true_branch = _generate_subtree(root[HIGH])
        if type(root[LOW]) is not dict:
            new_rule.false_branch = Leaf({int(root[LOW])-1})
            if HITS_LOW in root:
                class_hits = np.zeros(num_of_classes)
                class_hits[:len(root[HITS_LOW])] = root[HITS_LOW]
                new_rule.false_branch.class_hits = list(map(int, class_hits))
        else:
            new_rule.false_branch = _generate_subtree(root[LOW])
        if HITS_LOW in root and HITS_HIGH in root:
            class_hits = np.zeros(num_of_classes)
            class_hits[:len(root[HITS_LOW])] += root[HITS_LOW]
            class_hits[:len(root[HITS_HIGH])] += root[HITS_HIGH]
            new_rule.class_hits = list(map(int, class_hits))
        return new_rule

    result.root = _generate_subtree(pybliquedt)

    return result


def train_OC1(x: np.array, y: np.array, metric=None) -> RuleTree:
    try:
        from pyblique import ObliqueClassifier
    except:
        print("ERROR: OC1 can not be trained because pyblique package is missing. Package is avaliable on https://github.com/fantamat/pyblique/"
              "originaly on https://github.com/KDercksen/pyblique")
        return
    if metric:
        oc = ObliqueClassifier(metric)
    else:
        oc = ObliqueClassifier()
    data = np.zeros(x.shape[0], x.shape[1]+1)
    data[:, :-1] = x
    data[:, -1] = y + 1
    oc.fit(data)
    return pybliquedt_to_ruletree(oc.tree)


def train_class_wise(labels: np.array, function, all_others=False):
    """
    Build RuleTree for each class separately, classes are sorted by their occurrence.
    :param labels: labels of training data
    :param function: function that returns ruletree and dictionary with information as a parameter takes
        considered class index, training sample indexes
    :return: composed ruletree of the extracted trees and list of obtained additional information
    """
    DUMMY_SET = {"dummy"}
    num_classes = np.max(labels) + 1
    u,c = np.unique(labels, return_counts=True)
    ar = np.arange(len(labels))
    class_indexes = list()
    for i in range(num_classes):
        class_indexes.append(ar[labels==i])

    def get_train_data(class_index, others):
        index0 = class_indexes[class_index]
        index1 = list()
        for i in others:
            index1 += list(class_indexes[i])
        index1 = np.random.choice(index1, len(index0))
        return np.append(index0, index1, axis=0)


    l_order = np.argsort(c)
    other_l = list(u)
    out_inf = dict()
    out_rt = None
    for i in l_order[:-1]:
        if not all_others:
            other_l.remove(i)
        indexes = get_train_data(i, other_l)
        rt, inf = function(i, indexes)
        out_inf[i] = inf
        out_inf[i]["rt"] = rt.copy()

        new_leaf = Leaf({i})
        new_dummy_leaf = Leaf(DUMMY_SET)
        a = rt.get_all_nodes()
        p = rt.get_predecessor_dict(a)
        rt.replace_leaf_with_set({1}, new_dummy_leaf, all_nodes=a, prevs=p)
        rt.replace_leaf_with_set({0}, new_leaf, all_nodes=a, prevs=p)
        if not out_rt:
            all_nodes = rt.get_all_nodes()
            prevs = rt.get_predecessor_dict(all_nodes)
            out_rt = rt.copy()
        else:
            out_rt.replace_leaf_with_set(DUMMY_SET, rt.root, all_nodes=all_nodes, prevs=prevs)
            all_nodes = all_nodes | a
            prevs = prevs.update(p)
    final_leaf = Leaf({l_order[-1]})
    out_rt.replace_leaf_with_set(DUMMY_SET, final_leaf, all_nodes=all_nodes, prevs=prevs)

    return out_rt, out_inf
